\clearpage{\pagestyle{empty}\cleardoublepage}

\chapter{Searches for vector-like top partner pairs in the single lepton channel}~\label{chap:vlq}

Starting from this chapter, and continuing in Chapter~\ref{chap:wbx} 
and Chapter~\ref{chap:htx}, we are going to describe two 
searches for pair production of vector-like top quark 
partners (\TTbar) performed in the 
single-lepton\footnote{In the following, with the 
word ``lepton'' we will 
refer to either an electron or a muon, assumed to 
originate from the decay of a $W$ boson or a $\tau$ lepton.}
%with its associated neutrino, which is considered to be the only particle contributing to the transverse missing energy \met.}
 channel. These analyses
are optimized for different \T\ quark decay modes and are thus complementary.
The first search focuses on  \T\ quark decay channels with high 
branching ratio to 
a $W$ boson and a bottom quark, while the second search is optimized for events with 
high branching ratio to a Higgs boson and a top quark. 
%and is performed using the full dataset of pp collisions at the \cme\ of \rts=8~\tev\ collected during 2012 at the ATLAS detector, consinsting in 20.34~\ifb, while
%search for vector-like top partners with high branching ratio to $Ht$
%uses a partial dataset of the same data, amounting to 14.3~\ifb.
The analyses are performed using most
of the dataset of pp collisions at
\rts=8~\tev\ collected during 2012
by the ATLAS detector, corresponding to an integrated luminosity of  14.3~\ifb.

This chapter is devoted to the presentation of the general features that are common to 
the two analyses and is organized as follows.
Section~\ref{sec:strategy}
describes the strategy for vector-like quark searches adopted 
by the Exotics group of the ATLAS collaboration.
Section~\ref{sec:presel}
summarises the common event preselection for data and few general concepts in the
design of the analyses. Section~\ref{sec:datasets}
describes the Monte Carlo samples used, which
are in general common to both analyses with only few exceptions,
and how the multi-jet background is estimated. 
Finally, Section~\ref{sec:systematics} 
describes the general treatment of systematic uncertainties
and Section~\ref{sec:cls} explains the concepts of the statistical analysis.
%The two analyses are then presented in details in Chapter~\ref{chap:wbx} 
%(\TTbar\ pairs decaying to \wbx\ ) and in Chapter~\ref{chap:htx}
%(\TTbar\ pairs decaying to \htx\ ). The final results are presented 
%in Chapter~\ref{chap:results}.

\section{General strategy for vector-like quark pairs searches}\label{sec:strategy}

The phenomenology for vector-like quarks was described in Section~\ref{sec:THvlq}.
Here we will briefly review the basic concepts on which
the strategy for the searches has been built. Table~\ref{tab:vlqdecays} summarizes the 
decay modes for vector-like quarks in the singlet and doublet models. It is evident
from the richness of the possible decay modes, combined with the unpredicted mass
of the heavy objects that could span from few hundreds \gev s up to  
$\sim 1$~\tev\footnote{This mass interval derives from the results of previous searches,
which excluded low mass values, and from the fact that we focus on pair production of vector-like
quarks, which is favoured up to the $\sim 1$~\tev\ mass scale.}% as shown in Figure~\ref{fig:vlqxsec}), 
that is impossible to cover it with a single inclusive search.

\begin{table}[htb]\centering
\begin{tabular}{|cc|cc|}\toprule
Singlet & Decay modes & Doublet & Decay modes\\
& & &\\
$T(+2/3)$ & $W^+b,\, Ht,\, Zt$ & \multirow{2}{*}{$\quad\bigg(\begin{array}{c}T \\ B\end{array}\bigg)$} & $W^+b,\, Ht,\, Zt$\\ 
& & & $ W^-t,\, Hb,\, Zb$\\
$B(-1/3)$ & $ W^-t,\, Hb,\, Zb$ & & \\
& & \multirow{2}{*}{$\quad\bigg(\begin{array}{c}T \\ X\end{array}\bigg)$} & $Ht,\, Zt$\\
$X(+5/3)$ & $W^+t$ & & $W^+t$\\
& & &\\
$Y(-4/3)$ & $W^-b$ & \multirow{2}{*}{$\quad\bigg(\begin{array}{c}B \\ Y\end{array}\bigg)$} & $Hb,\, Zb$\\
& & & $W^-b$\\\bottomrule
\end{tabular}
\caption{Allowed decay modes for vector-like singlets and doublets.}\label{tab:vlqdecays}
\end{table}

%\begin{figure}[htb]\begin{center}
%	\subfigure{
%  	\includegraphics[width=0.65\textwidth, angle=270]{vlq_analysis/figures/xsec-8}}
%	\caption{\label{fig:vlqxsec} Pair and single production cross sections  for heavy 
%        quarks in proton-proton collisions at $\sqrt{s}=8$~TeV~\cite{Aguilar-Saavedra:2013qpa}.}
%\end{center}\end{figure}

The branching ratio of vector-like top and bottom partners 
%(\T\ and \B, respectively) 
to the allowed decay modes depends
on the mass of the heavy quark and on the considered model (in our case, singlet or doublet
scenario), as shown in Figure~\ref{fig:vlqBRs}.
Considering that the exotic heavy quark $Y$ decays
exclusively into $W^{-}b$, one of the allowed
decay modes of the \T\ quark, and since
the searches that are presented are not
sensitive to the charge of the vector-like quark,
the results presented for the  \T\ quark are
valid also for the $Y$ quark. A similar
argument applies to the \B\ and $X$ quarks. 
%Considering that the exotic heavy quark $Y$ ($X$)
%decay into one of the \T\ (\B) decay channels
%and that the searches that are presented are not
%sensitive to the charge of the vector-like quark,
%with \T\ (\B) we will in general refer both to
%\T\ and $Y$ (\B\ and $X$). 
Each decay mode has specific features that allow to define powerful, optimized searches.
Therefore in order to exploit this potential and at the same time stay as model independent
as possible, different searches for vector-like quarks 
are performed, each of them being sensitive to specific decay modes.
These searches can eventually be combined to maximize 
the overall sensitivity.
To ensure a comprehensive coverage of the phase space, a two-dimensional plane is defined 
(Figure~\ref{fig:2dplane}) as follows:
along the Y axis is the branching ratio of the decay 
modes with a Higgs boson in the final state; along the X axis is the branching ratio 
of the decay modes with a $W$ boson in the final state.
The branching ratio to the channel with a $Z$ boson in the final state is then fixed by the 
unitarity requirement BR($T/B\to  Zt/b$) = 1 - BR($T/B\to Ht/b$) - BR($T/B\to Wb/t$).
A plane of this kind is defined for every vector-like quark mass point considered 
in the analysis. Each point of each plane therefore represents a 
uniquely defined model, and analyses are performed for every configuration
to either find deviations from expectations or to set a 95\% Confidence Level (CL) exclusion.
The final objective of the joint strategy is to cover the full plane by combining
analyses probing the different signatures.

\begin{figure}[h!tb]\begin{center}
	\subfigure[]{\label{fig:vltBRs}
  	\includegraphics[width=0.47\textwidth]{vlq_analysis/figures/fig_02a.eps}}
	\subfigure[]{\label{fig:vlbBRs}
  	\includegraphics[width=0.47\textwidth]{vlq_analysis/figures/fig_02b.eps}}
	\caption{Branching ratios for the decay of vector-like top (a) and bottom (b) partners as a function of the heavy quark mass $m_T$ and $m_B$ respectively~\cite{ATLAS-CONF-2013-056} for singlet and doublet models.\label{fig:vlqBRs}}
\end{center}\end{figure}

\begin{figure}[h!bt]\begin{center}
	\subfigure[]{
        \input{vlq_analysis/planeT}}
	\subfigure[]{
        \input{vlq_analysis/planeB}}
       	\caption{Two dimensional plane used to represent the comprehensive scan of model mixing, for (a) \T\ quark
        and (b) \B\ quark. Searches 
        with a Higgs boson in the final state cover the top left corner; searches with a $Z$ boson in the 
        final state cover the bottom left corner; searches with a $W$ boson in the final state cover the 
        bottom right corner. The shaded area labelled as ``forbidden'' is the unphysical region where
        BR($T/B\to Ht/b$) + BR($T/B\to Wb/t$) + BR($T/B\to  Zt/b$) $>1$ \label{fig:2dplane}}
\end{center}\end{figure}


As of the date of the writing of this dissertation, four complementary 
and quasi model-independent searches have been performed by the ATLAS Exotics working
group using 14.3~\ifb\ of pp collision data at 
$\sqrt{s}=8\tev$.  Briefly summarized below are
two analyses that investigated multi-lepton channels and two that probed 
single-lepton channels.

The search for \BB\ and \TT\ in the same-sign 
dilepton channel~\cite{ATLAS-CONF-2013-051}
investigates a channel
with very small contamination from Standard Model backgrounds and
is also sensitive to four-top production $pp\to t\bar{t}t\bar{t}$, 
either through the Standard Model process or a beyond-SM source such as 
pair production of scalar color-octets (sgluons) or gluinos, with subsequent 
decays to top quark pairs.
The approach of this search is to select via restrictive cuts (that also
impose a veto on a $Z\to \ell^+\ell^-$ boson) the eventual signal
and compare the final counts with the expected yields from background sources.
Figure~\ref{fig:feyndSS} illustrates typical decays of \BB\ and \TT\ 
that contribute to the same-sign dilepton signature. From this it is easy to
understand that this search will be mainly covering the bottom right corner
of the \BBbar\ two dimensional plane of Figure~\ref{fig:2dplane} and the top 
left corner of the same plane for \TTbar.
%Data-driven techniques are used to evaluate the background contribution to the signal region coming from charge misidentification and fakes, the two main sources of background. In the end 3, 10 and 2 events are observed in the $ee$, $e\mu$ and $\mu\mu$ channels respectively. The observed counts are consistent with the expected yields in the $ee$ and $\mu\mu$ channel. A small excess of events is observed in the $e\mu$ channel, leading to a slightly weaker limit than expected, but still within a $\pm1\sigma$ band.
%These counts are consistent with the expected yields, except for the  $e\mu$ channel.

\begin{figure}[hbt]
\begin{center}
        \myskip\input{vlq_analysis/figures/feyndiag_BB_ssdilep}\myskip
	\caption{Feynman diagrams for \BBbar\ (left) and \TTbar\ (right)
        decays that can result in a final state with two same-sign leptons,
        in the case that the same-sign $W$ bosons (highlighted in different colors;
        on the right the anti-top quarks associated to the $Z$ or Higgs bosons are
        also highlighted as they will originate a $W^{-}$ boson) decay
        into a lepton and a neutrino. \label{fig:feyndSS}}
\end{center}
\end{figure}


The search in the opposite-charge dilepton channel~\cite{ATLAS-CONF-2013-056} 
focuses on events where at least one
heavy quark  (\B\ or \T) decays into a $Z$ boson and a bottom or top quark. 
Here the strategy is to identify the $Z$ boson from the opposite-charge lepton
pair and to reconstruct the heavy quark mass 
by pairing the $Z$ boson candidate with the highest 
$p_T$ $b$-jet, used as final discriminating variable in the statistical analysis.
It is expected of this search to efficiently cover the bottom
left corners of both the \B\ and \T\ branching ratio planes of
Figure~\ref{fig:2dplane}.


The searches in the single lepton channel are focused on \TT\ 
decays and are optimized for two distinctive signatures. One search
exploits the boosted kinematics of the $W$ boson from \T\ decays
to reconstruct it from its hadronic channel final state 
particles~\cite{ATLAS-CONF-2013-060} (see Figure~\ref{fig:feyndTTwbx}). The heavy quark invariant mass is 
then reconstructed pairing the boosted  $W$ boson  with a \bjet\ and this
distribution is used to perform the statistical analysis. It is evident that this
search is going to cover the bottom right corner of the \T\  branching ratio plane 
of Figure~\ref{fig:2dplane}.

The other search in the single lepton channel considers final states with high
jet and \bjet\ multiplicities as a result of the decay of at least one heavy quark
into a Higgs boson (assumed to decay into \bbbar) and a top 
quark~\cite{ATLAS-CONF-2013-018} (see Figure~\ref{fig:feyndTThtx}). The distribution
of the scalar sum of transverse momentum in the event is then used to perform the statistical 
analysis. This search is mainly sensitive to the top left corner of the \T\  
branching ratio plane of Figure~\ref{fig:2dplane}.


\begin{figure}[hbt]
\begin{center}
        %\myskip\input{vlq_analysis/figures/feyndiag_TT_HtX}\myskip
        \myskip\input{vlq_analysis/figures/feyndiag_TT_vlq}\myskip
	\caption{(a) Feynman diagram for the $\TTbar\to Ht+X$
        decay entering the high jet and \bjet\ multiplicity final states.
        Assuming the single lepton condition, in this picture all the bosons 
        produced in the $\Tbar$ decay will decay hadronically.
        (b) Feynman diagram for the $\TTbar\to WbWb$ decay.\label{fig:feyndHTX}}
\end{center}
\end{figure}



In the following we will discuss in details the two searches for 
\TT\ performed in the single lepton channel,
referred to as \wbx\ and \htx\ respectively, starting with the
discussion of the common features between the two analyses.


\section{Data sample and common event preselection}\label{sec:presel}

The pp collision data recorded by the ATLAS experiment during
2012 at a \cme\ of $\rts=8\tev$ are considered. Physics object definitions 
were discussed in Chapter~\ref{chap:objects}.
Events collected during
stable beam periods are required to pass data quality requirements and
single lepton trigger selection. In order to maximize trigger
efficiency, different transverse momentum threshold triggers are combined
through a logical \OR, with the lower \pt\ ones including isolation requirements
that result in inefficiencies for high \pt\ lepton candidates, recovered with
the use of the higher threshold triggers and 
no isolation requests. Single electron triggers have
\pt\ thresholds of 24 and 60~\gev, while
single muon triggers have \pt\ thresholds  
of 24 and 36~\gev\ (Section~\ref{sec:REQtrigger}).

After passing trigger requirements, events with more than one lepton are
discarded. In addition, the lepton selected offline is required
to match within $\dr<0.15$ the corresponding lepton at the
trigger level. Furthermore, at least four jets satisfying the conditions
described in Section~\ref{sec:jets} are required, at least one of them
being tagged as a \bjet.

In order to suppress the QCD multi-jet background,
combined cuts on the \met\ and on the tranverse mass of the 
leptonically-decaying $W$ boson \mt\footnote{$\mt = \sqrt{2 p^\ell_{\rm T} \met (1-\cos\Delta\phi)}$, with
$p^\ell_{\rm T}$  being the transverse momentum (energy) of the 
muon (electron) and $\Delta\phi$ the
azimuthal angle separation between the lepton and the direction of
the missing transverse momentum.}\ 
are defined: $\met>20~\gev$ and $\met+\mt>60~\gev$.
These cuts constitute the event preselection and are summarized in
Table~\ref{tab:preselcuts}.

\begin{table}[tb]
\begin{center}
\begin{tabular}{ll}
\toprule
Preselection stage & Requirements \\
\midrule
Single lepton & One electron or muon matching trigger  \\
QCD rejection & $\met >20\gev$, $\met +m_{\rm T}>60\gev$ \\
Jet multiplicity & $\geq 4$ jets, $\geq 1$ $b$-tagged jets \\
\bottomrule\end{tabular}\caption{Summary of the cuts applied for 
event preselection, common to the \wbx\ and \htx\ analyses.}\label{tab:preselcuts}
\end{center}
\end{table}


At this point, a simple consideration about the typical expected jet
(and \bjet) multiplicity based on counting the parton multiplicities and their 
flavor is made so as to define an orthogonality
cut between the two analyses. Table~\ref{tab:jetmult} shows the expected
number of jets (\bjet s) per decay channel combinations of \TTbar\ pairs, 
in the case of single lepton selection with at least four jets
(i.e. one $W$ boson will always decay into lepton and neutrino,
and $Z$ boson decay to neutrinos is excluded in the $WbZt$ channel) and assuming that
the Higgs boson decays to \bbbar.
To avoid overlap between selected events from the two analyses, in the
\wbx\ analysis events with $\geq$6 jets and $\geq$3 \bjet s are 
rejected\footnote{As will be explained in Section~\ref{sec:htxEVT}, another orthogonality
cut will be applied in the low \bjet\ multiplicity channel of the \htx\ analysis.}.

\begin{table}\centering
\begin{tabular}{lccc}\toprule
& $Wb$ & $Ht$ & $Zt$ \\\midrule
&\cellcolor{lightgray} & & \cellcolor{lightgray}\\
\multirow{-2}{*}{$Wb$} & 
\cellcolor{lightgray}\multirow{-2}{*}{\bf 4 (2)} & 
\multirow{-2}{*}{6 (4)} & 
\cellcolor{lightgray}\multirow{-2}{*}{{\bf 6} ({\bf2}/4)} \\
\multirow{2}{*}{$Ht$} & 
\multirow{2}{*}{6 (4)} & 
\multirow{2}{*}{8 (6)} & max: 8 (4/6)\\
& & & \cellcolor{lightgray}min: {\bf6 (2)}\\
\multirow{2}{*}{$Zt$} & 
\cellcolor{lightgray}& max: 8 (4/6) & 
\cellcolor{lightgray}max: {\bf8} ({\bf2}/6) \\
& \cellcolor{lightgray}\multirow{-2}{*}{\bf6 (2/4)} & 
\cellcolor{lightgray}min: {\bf6 (2)} & 
\cellcolor{lightgray}min: {\bf6} ({\bf2}/4)\\
\bottomrule\end{tabular}\caption{Expected jets (\bjet) multiplicities in 
the various possible final states. $Z$ boson decays 55\% hadronically, 
15\% of the times into \bbbar, therefore the min/max number of \bjet s 
is reported. Highlighted in bold characters are the channels that after the orthogonality 
cut will contribute to the \wbx\ analysis.}\label{tab:jetmult}
\end{table}




\section{Background and signal modeling}\label{sec:datasets}

Signal and background samples are 
modeled using Monte Carlo simulation, with the exception
of QCD multi-jet background, which is estimated 
using data-driven techniques (see Section~\ref{sec:qcdbkg}). 
%, and background from $W$ production in association with jets ($W$+jets), which is obtained
%from Monte Carlo at first but then is normalized to data.

The main background for both analyses is $t\bar{t}$ 
production %in association with jets ($t\bar{t}$+jets)
and different choices for the generator are made
in the analyses because of the very different
topologies for \ttbar\ production being probed by
each analysis. Indeed, the \wbx\ search will select \ttbar\ events
with high-\pt\ objects, while the \htx\ search will
select \ttbar\ events produced in association with
multiple heavy flavor jets.
In the case of the $t\bar{t}$+jets background prediction for the \htx\ analysis 
further corrections to match the data are applied, due to a mismodeling in the
heavy- and light-flavour content of the simulated sample 
(see Section~\ref{sec:htxEVT}).

Background from $W$+jets 
and QCD multi-jet events also contributes, the latter
entering into the event selection via the misidentification 
of a jet or a photon as an
electron or the presence of a 
non-prompt lepton from, e.g., semileptonic $b$- or $c$-hadron decay.
In the case of $W$+jets background, the kinematics is modeled
with the simulation, while the overall normalization and flavor content
is estimated from data (see Section~\ref{sec:Wjetsnorm}).
Small background contributions originate from single top quark, $Z$+jets, diboson
($WW,WZ,ZZ$), and associated $t\bar{t}V$ ($V=W,Z$) and $t\bar{t}H$ production.

\subsection{Monte Carlo simulated samples}\label{sec:MCbkg}

%All event generators using {\tt HERWIG}~\cite{HERWIG} are also interfaced to {\tt JIMMY v4.31}~\cite{jimmy} to simulate the underlying event.  
With the exception of the 
signal samples, all simulated 
samples utilise {\tt PHOTOS 2.15}~\cite{PhotosPaper} to model
photon radiation and {\tt TAUOLA 1.20}~\cite{TauolaPaper} to model
$\tau$ decays.  

All simulated samples include multiple pp
interactions and make use of the  {\tt GEANT4}~\cite{geant}
detector geometry and response simulation~\cite{atlas_sim}
with the exception of the signal samples, for which a fast simulation of
the calorimeter response is used.
All simulated samples are then processed through the same reconstruction 
software as the data and are reweighted to match 
the instantaneous luminosity profile in data. For more details
on the Monte Carlo simulation chain see Section~\ref{sec:generators}.

%Additional corrections are applied so that the 
%object identification efficiencies, energy
%scales and energy resolutions match those determined in data control
%samples.


\subsubsection{$t\bar{t}$ MC@NLO}\label{subsec:MC@NLO}
Simulated samples of $t\bar{t}$ pair production  in association with jets 
($t\bar{t}$+jets or simply $t\bar{t}$ in the following)
are generated with {\tt MC@NLO} v4.01~\cite{mcatnlo_1,mcatnlo_2,mcatnlo_3} using the {\tt CT10} set of parton distribution functions (PDFs)~\cite{ct10},
with parton-shower and fragmentation modelled with
{\tt HERWIG} v6.520~\cite{HERWIG}.
The top quark mass is assumed to be  $172.5\gev$ and 
the samples are normalized to approximate next-to-next-to-leading-order 
(NNLO) theoretical cross section~\cite{ttbarxs}; the cross section used 
has been computed with {\tt HATHOR 1.2}~\cite{ttbarxs} using the {\tt MSTW2008}
NNLO PDF set~\cite{Martin:2009iq} and is $\sigma_{t\bar{t}}= 238^{+22}_{-24}$~pb, 
where the total uncertainty results from the sum in quadrature of the 
scale and PDF+$\alpha_S$ uncertainties according to 
the {\tt MSTW} prescription~\cite{mstw2}. 
These samples are used in the \wbx\ analysis.

\subsubsection{$t\bar{t}$ ALPGEN}\label{subsec:alpgen}
Additional simulated samples of $t\bar{t}$+jets are generated using
%and $W/Z$+jets events are generated using
the {\tt ALPGEN v2.13}~\cite{ALPGEN} leading-order (LO) generator and the 
{\tt CTEQ6L1} PDF set~\cite{cteq6}, with parton shower and fragmentation  
modelled with {\tt HERWIG} v6.520~\cite{HERWIG}.

Separate samples are generated for $t\bar{t}$+light jets ($t\bar{t}$+light 
or $t\bar{t}$+LF in the following, for ``light flavour'') 
with up to three additional light partons ($u$, $d$, $s$ quarks or gluons),
and for $t\bar{t}$+heavy-flavour jets ($t\bar{t}$+HF in the following), 
including $t\bar{t}b\bar{b}$ and
$t\bar{t}c\bar{c}$.  
The MLM parton-jet matching scheme~\cite{mlm} is used
in order to avoid double-counting  of partonic configurations
generated both at the matrix-element calculation level
and at the parton-shower evolution step.
An algorithm based on the angular separation
between the extra heavy quarks is used to remove 
the overlap between $t\bar{t}Q\bar{Q}$ ($Q=b,c$) 
generated from the matrix element calculation and 
from parton-shower evolution in the  $t\bar{t}$+light samples: 
the matrix-element prediction is chosen over the parton-shower one
only when $\Delta R(Q,\bar{Q})>0.4$.

%The algorithm used is implemented in the HFOR tool~\cite{hfor}.

Again a top quark mass of $172.5\gev$ is assumed, and normalisation to the
approximate NNLO theoretical cross section is used.
These samples are used in the \htx\ analysis.


\subsubsection{$W/Z$+jets}

Simulated samples of $W/Z$ boson production in association with jets
($W/Z$+jets in the following) are generated with up to five additional 
partons using the {\tt ALPGEN v2.13}~\cite{ALPGEN} LO generator and the 
{\tt CTEQ6L1} PDF set~\cite{cteq6}, interfaced to {\tt HERWIG} v6.520 
for parton showering and fragmentation.
The MLM matching scheme is used also here to avoid double-counting of partonic configurations 
between  matrix-element  calculation and parton showering.

The $W$+jets samples are generated separately for $W$+light jets, 
$Wb\bar{b}$+jets, $Wc\bar{c}$+jets, and $Wc$+jets, 
with the relative contributions normalized using the fraction 
of $b$-tagged jets in $W$+1-jet and $W$+2-jets data 
control samples~\cite{whf} (see Section~\ref{sec:Wjetsnorm}), while
the $Z$+jets samples are generated separately 
for $Z$+light jets, $Zb\bar{b}$+jets, and $Zc\bar{c}$+jets and
normalized to the inclusive NNLO theoretical cross section~\cite{vjetsxs}.
Overlap between $W/ZQ\bar{Q}$+jets ($Q=b,c$) 
events generated from the matrix element calculation and those
generated from parton-shower evolution in the $W/Z$+light jets
samples is avoided via the same algorithm used
for $t\bar{t}$ \texttt{ALPGEN}.

\subsubsection{Other backgrounds}\label{subsec:otherbkg}
%,tchanxs,Wtchanxs,schanxs}. 
Simulated samples of single top quark backgrounds corresponding to the
$s$-channel and $Wt$ production mechanisms are generated with {\tt
MC@NLO} v4.01~\cite{mcatnlo_1,mcatnlo_2,mcatnlo_3} using the {\tt
CT10} PDF set~\cite{ct10}.  In the case of $t$-channel single top
quark production, the {\tt ACERMC v3.8} LO generator~\cite{acermc}
with the {\tt MRST LO**} PDF set is used.

Simulated samples of $t\bar{t}$ produced in association with a $W$ or $Z$ boson
($t\bar{t}V$ $(V=W,Z)$ in the following) are generated with the {\tt MADGRAPH v5} LO
generator~\cite{madgraph} and the {\tt CTEQ6L1} PDF set.  

Samples of $t\bar{t}$ produced in association with a Higgs boson
($t\bar{t}H$ in the following) are generated with the 
{\tt PYTHIA} 6.425~\cite{py6} LO generator and the {\tt MRST LO**} PDF set~\cite{mrst},
assuming a Higgs boson mass of $125\gev$ and considering the 
$H\to b\bar{b}$, $c\bar{c}$, $gg$, and $W^+W^-$ decay modes.

Parton shower and fragmentation are modelled with {\tt HERWIG}
v6.520~\cite{HERWIG} in the case of {\tt MC@NLO}, with {\tt PYTHIA}
6.421 in the case of {\tt ACERMC}, and with {\tt PYTHIA 6.425} in the
case of {\tt MADGRAPH}.  All these samples are generated assuming a top
quark mass of $172.5\gev$. The single top quark samples are normalised to
the approximate NNLO theoretical cross sections~\cite{stopxs,stopxs_2}
using the {\tt MSTW2008} NNLO PDF set, while the $t\bar{t}V$ samples
are normalised to the NLO cross section predictions~\cite{ttbarVxs1,ttbarVxs2}.
The $t\bar{t}H$ sample is normalised using the NLO theoretical cross section 
and branching ratio predictions~\cite{lhcxs}.
Finally, the diboson backgrounds are modelled using {\tt HERWIG} with
the {\tt MRST LO**} PDF set, and are normalised to their NLO
theoretical cross sections~\cite{dibosonxs}.

\subsubsection{Signal samples}\label{subsec:MCsignal}


Samples of \TTbar\ signal, corresponding to a singlet $T$ quark 
decaying to $Wb$, $Zt$ and $Ht$ are generated with the {\tt PROTOS v2.2} 
LO generator~\cite{AguilarSaavedra:2009es,protos} 
using the  {\tt MSTW2008} LO PDF set, and interfaced to {\tt PYTHIA} for 
the parton shower and fragmentation. 

For each decay channel ($Wb$, $Zt$ and $Ht$) the branching ratio has been 
set to 1/3. Events are reweighted at the analysis level
in order to reproduce any desired branching ratio configuration. 
The predicted branching ratios in the weak-isospin singlet and doublet scenarios as 
a function of $m_{T}$ are given in Table~\ref{tab:BRT}.

The $m_{T}$ values considered range from $350\gev$ to $850\gev$ in steps of $50\gev$, 
with the Higgs boson mass assumed 
to be $125\gev$. All Higgs boson decay modes are considered, 
with branching ratios as predicted by {\tt HDECAY}~\cite{hdecay}.

Signal samples are normalized to the approximate NNLO theoretical cross sections~\cite{ttbarxs} using the {\tt MSTW2008} NNLO PDF set.
The cross section values used are summarized in Table~\ref{tab:sigmaTT}.



\begin{table}[h!]
\begin{center}
\resizebox{1.\textwidth}{!}{
\begin{tabular}{c c c c c c c}
\toprule
 & \multicolumn{3}{c}{Singlet} &  \multicolumn{3}{c}{Doublet} \\
 $m_{T}$ ($\gev$) & $BR(T \to Wb)$ & $BR(T \to Zt)$ & $BR(T \to Ht)$ & $BR(T \to Wb)$ & $BR(T \to Zt)$ & $BR(T \to Ht)$\\
\midrule
350 	&  0.545 	&  0.116 	&  0.338	&  0.000 	&  0.255 	&  0.745 	\\ 
400 	&  0.513 	&  0.139 	&  0.348	&  0.000 	&  0.285 	&  0.715 	\\
450 	&  0.502 	&  0.158 	&  0.341	&  0.000 	&  0.316 	&  0.684 	\\ 
500 	&  0.497 	&  0.173 	&  0.330	&  0.000 	&  0.343 	&  0.657 	\\
550 	&  0.495 	&  0.185 	&  0.321	&  0.000 	&  0.365 	&  0.635 	\\
600 	&  0.494 	&  0.194 	&  0.312	&  0.000 	&  0.383 	&  0.617 	\\ 	
650 	&  0.494 	&  0.202 	&  0.304	&  0.000 	&  0.399 	&  0.601 	\\ 
700 	&  0.494 	&  0.208 	&  0.298	&  0.000 	&  0.411 	&  0.589 	\\ 
750 	&  0.494 	&  0.214 	&  0.292	&  0.000 	&  0.422 	&  0.578 	\\ 
800 	&  0.494 	&  0.218 	&  0.288	&  0.000 	&  0.431 	&  0.569 	\\
850 	&  0.494 	&  0.222 	&  0.284	&  0.000 	&  0.439 	&  0.561 	\\ 
\bottomrule
\end{tabular}}
\caption{\label{tab:BRT} Branching ratios for $T$ decay as a function
of $m_{T}$ as computed with {\tt PROTOS} in the weak-isospin singlet and doublet scenarios.
The same values are used in the graphical representation of Figure~\ref{fig:vlqBRs}}
\end{center}
\end{table}
%%%%%%%%
\begin{table}[h!]
\begin{center}
\resizebox{1.\textwidth}{!}{
\begin{tabular}{c c c c c}
\toprule
 $m_{T}$ ($\gev$) & $\sigma(\TTbar)$ (pb) & Scale uncertainties (pb) & PDF+$\alpha_s$ uncertainties (pb) & Total uncertainty (pb)\\
\midrule
350 	&  5.083 		&  +0.140/-0.285 		&  + 0.569/-0.488 		&  +0.586/-0.565		\\
400 	&  2.296 		&  +0.066/-0.130 		&  + 0.269/-0.221 		&  +0.277/-0.257		\\
450 	&  1.113 		&  +0.034/-0.063 		&  + 0.136/-0.107 		&  +0.140/-0.125		\\
500 	&  0.5702 		&  +0.0185/-0.0327 		&  + 0.0723/-0.0545	 	&  +0.0746/-0.0636		\\
550 	&  0.30545 	&  +0.01040/-0.01769 	&  + 0.04012/-0.02889 	&  +0.0414/-0.0339		\\
600 	&  0.1696 		&  +0.0060/-0.0099 		&  + 0.0230/-0.0161	 	&  +0.0238/-0.0189		\\	
650 	&  0.09707 	&  +0.00359/-0.00571 	&  + 0.01363/-0.00936 	&  +0.01410/-0.01097	\\
700 	&  0.05694 	&  +0.00218/-0.00338 	&  + 0.00828/-0.00559 	&  +0.00856/-0.00653	\\
750 	&  0.03411 	&  +0.00135/-0.00204 	&  + 0.00513/-0.00343 	&  +0.00530/-0.00400	\\
800 	&  0.02080 	&  +0.00085/-0.00126 	&  + 0.00329/-0.00216 	&  +0.00340/-0.00250	\\
850 	&  0.01287 	&  +0.00054/-0.00079 	&  + 0.00215/-0.00138 	&  +0.00222/-0.00159 	\\
\bottomrule
\end{tabular}}
\caption{\label{tab:sigmaTT} Theoretical cross section at NNLO  for \TT\ production 
as a function
of $m_{T}$ as computed by {\tt HATHOR}, and corresponding scale and PDF uncertainties.}
\end{center}
\end{table}
%%%%%%%%



\subsection{Multi-jet background}\label{sec:qcdbkg}

QCD multi-jet production can pass the event selection in the electron
channel via non-prompt electrons from e.g. heavy flavor quark decays, via 
photons mis-identified as electrons in the case of $\gamma\to e^+ e^-$
conversion or random track overlap,
or via jets mis-identified as electrons
because they left a high amount of energy in the electromagnetic calorimeter
like in the case of $\pi^0$ and $\eta$ decays into two close-by photons.
For events in the muon channel the main contributions come from
non-prompt leptons from semileptonic $b$- and $c$-hadron decays.

Although these kind of events rarely pass the quality cuts 
required at the lepton reconstruction stage, the production cross section
is so high (orders  of magnitude more than \ttbar\ production)
that the contribution to the background from QCD multi-jet events is
no longer negligible. The QCD multi-jet contribution
is estimated via data-driven 
methods, since simulation is not expected to predict this contribution
with the desired level of accuracy.

The technique used is called ``Matrix Method'' (MM in the following)~\cite{ttbar_3pb}.  
The basic principle is to divide the data sample into two categories, one
of events passing the standard selection criteria (``tight'' events), the
other including also leptons satisfying looser requirements (``loose'' events). 
%Loose leptons can reasonably  be considered as either {\it real} leptons or {\it fake} leptons, and it can be assumed that most of the real leptons will pass the tight selection. A good evaluation of multi-jet contamination is then that fraction of fake events going through the tight requirements. 
A pictorial representation of the sampling space 
is shown in Figure~\ref{fig:mmspace}. Typically tight lepton selection requirements
are the same as the ones used in the analysis (Sections~\ref{sec:electrons}
and~\ref{sec:muons}) where then some cuts are either removed or changed
to obtain the loose sample. In the case of muons, loose muons have the same
requirements as the preselected  
muons but without the mini-isolation cut. Loose electrons  have the same
requirements as the preselected  
electrons but the \texttt{tight++} requirement is replaced
by the \texttt{medium++} one, isolation requirements are omitted and
a condition to veto against conversion is added.

\begin{figure}[htb]\begin{center}
	\subfigure[]{\label{fig:mmspace}
  	\includegraphics[width=0.4\textwidth]{vlq_analysis/figures/mm_regions}}
	\caption{The events passing loose selection criteria can be real or fake leptons.
        Tighter requirements are added to the loose selection ones to define a sample of
        tight event which will contain real leptons as well as multi-jet background events.}
\end{center}\end{figure}

Defining $N^\mathrm{loose}_\mathrm{real}$ ($N^\mathrm{loose}_\mathrm{fake}$) as the number of
real (fake) leptons events satisfying the loose selection requirements, and
$N^\mathrm{tight}_\mathrm{real}$ ($N^\mathrm{tight}_\mathrm{fake}$) as the number of
real (fake) leptons events satisfying the tight selection requirements, we can write:

\begin{eqnarray}
\label{eqn:intro-mm-Nloose}
N^\mathrm{loose} & = & N^\mathrm{loose}_\mathrm{real} + N^{loose}_\mathrm{fake}, \\
\label{eqn:intro-mm-Ntight}
N^\mathrm{tight} & = & \epsilon_\mathrm{real}N^\mathrm{loose}_\mathrm{real} + \epsilon_\mathrm{fake}N^\mathrm{loose}_\mathrm{fake}.
\end{eqnarray}
where $\epsilon_\mathrm{real}$ ($\epsilon_\mathrm{fake}$) is the 
{\it efficiency} of selecting real (fake) loose leptons as tight leptons, i.e.:
\begin{eqnarray}
\label{eqn:intro-mm-real}
\epsilon_\mathrm{real} & = & \dfrac{N^\mathrm{tight}_\mathrm{real}}{N^\mathrm{loose}_\mathrm{real}}, \\
\label{eqn:intro-mm-fake}
\epsilon_\mathrm{fake} & = & \dfrac{N^\mathrm{tight}_\mathrm{fake}}{N^\mathrm{loose}_\mathrm{fake}}.
\end{eqnarray}


The number we are interested in to estimate the background from
multi-jet events is the amount of fake leptons contaminating the 
tight selection region, which can be obtained from
the previous equations as:

\begin{equation}
N^\mathrm{tight}_\mathrm{fake} = \frac{\epsilon_\mathrm{fake}}{\epsilon_\mathrm{real} - \epsilon_\mathrm{fake}}(N^\mathrm{loose} \epsilon_\mathrm{real} - N^\mathrm{tight}).
\label{eqn:intro-mm-tight_fake}
\end{equation}

An important condition for this method to work is that 
$\epsilon_\mathrm{real} \gg \epsilon_\mathrm{fake}$, which 
holds for the loose and tight lepton definition used
as $\epsilon_\mathrm{real}\sim 1$, while $\epsilon_\mathrm{fake}$
is in general well below 1.
Efficiencies for fake and real leptons to pass the 
tight requirement are measured in dedicated control regions, 
enriched respectively with fake and real leptons.
The efficiency $\epsilon_\mathrm{fake}$ is measured in control regions
with small \met\ and \mtw\ (a triangular cut on the
sum of the two variables is applied), which is
enriched in multi-jet background. In contrast,
the efficiency $\epsilon_\mathrm{real}$ is in general measured 
in regions with high \met\ or \mtw\
to select leptons from a leptonic decay of a $W$
boson.
While $\epsilon_\mathrm{real}$ shows basically
no dependence on the event topology and has
a stable value close to unity,  $\epsilon_\mathrm{fake}$ 
needs to be parametrized 
as a function of various kinematic variables
and ranges between 0.2 and 0.5.

In Appendix~\ref{app:qcdmm} the MM used for the estimation
of multi-jet background in the muon channel is
described in some more details, as the author of this dissertation
directly contributed to its development.


\subsection{$W$+jets background normalisation}\label{sec:Wjetsnorm}
 
For the $W$+jets background, a normalisation from data for the shapes 
obtained from the simulation is derived since the simulation 
overestimates the number of $W$+jets events
by up to $\sim$20\%, depending on the jet multiplicity.
Data-driven techniques are also used to correct the heavy flavor 
composition of the $W$+jets events, such that at the end the scale
factors applied are the product of the overall normalization scale factor
and the scale factors obtained for $Wb\bar{b}$, $Wc\bar{c}$, $Wcj$ and $Wjj$
components.

The density of quarks and anti-quarks inside the proton are different, hence 
the production of $W$ bosons in pp collisions will have different 
cross sections for processes like
$\sigma(u\bar{d}\to W^+)$ and $\sigma(d\bar{u}\to W^-)$. This charge
asymmetry in \wjets\ production is predicted theoretically~\cite{wasym}
and can be measured in data and then used to derive the correct
overall normalization of the process.
The total number of $W$+jets events in data $N_W=N_{W^+}+N_{W^-}$
is estimated measuring the difference between the number 
of positively- and negatively-charged $W$
bosons $(N_{W^+}-N_{W^-})_{\rm meas}$ (which basically is
the difference between positively- and negatively-charged leptons)
and compared to the prediction from Monte Carlo simulation:
\begin{equation}
N_W = \left(\frac{N_{W^+}+N_{W^-}}{N_{W^+}-N_{W^-}}\right )_{\rm MC}(N_{W^+}-N_{W^-})_{\rm meas}.
\label{eq:nw}
\end{equation}

Equation~\ref{eq:nw} is evaluated in the signal region (which for 
the general ATLAS top searches has at least 4 jets), 
without requirements on the \btag ging multiplicity.
%In order to estimate the fraction of event with 
%at least one \btag ged jet, 
%Equation~\ref{eq:nw} is multiplied 
% by the tagging fractions in the different
%jet bins evaluated in data by subtracting the Monte Carlo predictions of
%non-\wjets\ backgorunds.
To account for the different flavor composition, additional scale factors are derived 
%in the 2 jets bin 
considering that the total number of tagged \wjets\ 
events in the $i$th jet bin is given by:
\begin{equation}\label{eq:nwt}
N^{W,{\rm tag}}_{i}  = 
N^{W,{\rm pretag}}_{i}
\sum_{x=b\bar{b}, c\bar{c}, c, light} F_{x,i}P_{x,i},
\end{equation}
where $F_x$ are the flavor fractions $N^{\rm pretag}_x/N^{\rm pretag}$ 
(which add up to unity for each jet bin) and 
$P_x$ are the \btag ging probabilities for each flavor 
type $x = b\bar{b}, c\bar{c}, c, light$.
Equation~\ref{eq:nwt} is evaluated in the \wjets\ enriched
region with $i=2$.
Assuming the same flavor fractions for  $b\bar{b}$ and $c\bar{c}$,
since the same diagrams are involved, three unknown parameters need
to be fitted from data ($F_{c\bar{c},2}, F_{c,2}, F_{light,2}$).
This is achieved splitting Equation~\ref{eq:nwt} 
for positively- and negatively-charged lepton events. 
The flavor scale factors
are obtained as $K_x = F^{\rm data}_x/F^{\rm MC}_x$, where
$F^{\rm MC}_x$ are the flavor fractions from simulation.
The scale factors obtained for $W+2$jets events 
are then extrapolated to the
signal region with at least 4 jets by renormalizing to
unity the sum: 
\begin{equation}\label{eq:sfwn}
%\sum_{\rm x\ flavor} K_{x, {\rm 2\ jets}} F_{x, i\ {\rm jets}}^{\rm MC} = A, \qquad \neq 1 {\rm\ if\ } i \neq 2.
\sum_{x = b\bar{b}, c\bar{c}, c, light} K_{x,2} F_{x, i}^{\rm MC} = A, \qquad \neq 1 {\rm\ if\ } i \neq 2.
\end{equation}

%Events are categorised in terms of  multiplicity of $b$ and $c$ 
%jets and scale factors are
%derived using Equation~\ref{eq:nw}.
%The fraction of $W$+light jets events is scaled accordingly
%in order to preserve the overall normalisation of the $W$+jets background before $b$ tagging.





\section{Comparison between data and prediction}\label{sec:datamcpresel}

\input{vlq_analysis/datamcpresel}


\section{Systematic uncertainties}\label{sec:systematics}

In addition to the uncertainty that comes from the stochastic nature
of events we are subject to other sources that are systematical. 
% in the sense that they will bias the result towards a definite direction. 
They can originate from
detector measurements, from the way we reconstruct the objects, from the Monte
Carlo modelling, and can affect either only the normalization of the total event
yield (referred to as ``normalization-only'' systematic uncertainties) or also the shape 
of the distributions (referred to as ``shape and normalization'' systematic uncertainties).

The individual sources of systematic uncertainties are treated as uncorrelated, while
for each source the correlations are 
kept across processes and channels. Most systematic uncertainties
are common to the two analyses presented with only minor differences
that are discussed below.
The full list of systematic uncertainties considered is presented in 
Table~\ref{tab:SystSummary}, labelling them as  ``normalization-only'' 
or ``shape and normalization'' systematic uncertainties and indicating the number of components.

\begin{table}[htb]
\centering
\begin{tabular}{lcccc}
\toprule
Systematic uncertainty & \multicolumn{2}{c}{ \wbx\  } & \multicolumn{2}{c}{ \htx\  }\\
 & Status  & Components & Status  & Components\\
\midrule
Luminosity                  &  N & 1 &  N & 1\\
Lepton ID+reco+trigger      &  N & 1 &  N & 1\\
Jet vertex fraction efficiency & SN & 1 & SN & 1\\
Jet energy scale            & SN & 1 & SN & 8\\
Jet energy resolution       & SN & 1 & SN & 1\\
%Jet mass scale               & - & -\\
%Jet mass resolution      & - & -\\
$b$-tagging efficiency      & SN & 9 & SN & 9\\
$c$-tagging efficiency      & SN & 5 & SN & 5\\
Light jet-tagging efficiency    & SN & 1 & SN & 1\\
$t\bar{t}$ cross section    &  N & 1 &  N & 1\\
$t\bar{t}V$ cross section   &  N & 1 &  N & 1\\
$t\bar{t}H$ cross section   & - & - &  N & 1\\
Single top cross section    &  N & 1 &  N & 1\\
Dibosons cross section      &  N & 1 &  N & 1\\
$W$+jets normalization      &  N & 5 &  - & -\\
$Z$+jets normalization      &  N & 1 &  - & -\\
$V$+jets normalization      &  - & - &  N & 1\\
Multijet normalization      &  - & - &  N & 1\\
$t\bar{t}$ modelling        & SN & 3 & SN & 3\\
$V$+jets modelling         & SN & 1 &  - & -\\
$t\bar{t}$+heavy-flavour fractions &  - & -& N & 1\\
%$\TT$ modelling        & - & -\\
\bottomrule
\end{tabular}
\caption{\label{tab:SystSummary} 
List of systematic uncertainties considered in the two analyses. 
We label as ``N'' (``SN'') uncertainties taken as ``normalization-only'' 
(both ``shape'' and ``normalization'')
for all processes and channels. 
Some of the systematic uncertainties are split into several 
components for a more
accurate treatment. }
\end{table}

A discussion of the  systematic uncertainties considered in
both analyses is given below.
Details on specific treatments of systematic uncertainties in particular channels will
be given in the dedicated sections of the corresponding analysis chapters 
(Sections~\ref{sec:wbxSYS} and~\ref{sec:htxSYS}).



\subsection{Luminosity}
\label{sec:syst_lumi}
The uncertainty on the absolute integrated luminosity is estimated to be
of 3.6\%~\cite{lumi}. This systematic uncertainty
is applied to all processes except the QCD multi-jet background.

\subsection{Object definitions}
\label{sec:syst_objects}
The event reconstruction introduces uncertainties on the definition of
leptons, jets and on the $b$-, $c$-, and light flavour-tagging. In the
following the related systematic uncertainties considered are described.

\subsubsection{Lepton reconstruction, identification and trigger scale factors}
\label{sec:syst_lepID}

In Sections~\ref{sec:electrons} and~\ref{sec:muons} the reconstruction
of leptons was introduced, explaining the need of correcting for small
differences between data and simulation in the efficiency for
reconstruction, identification and trigger. This is done by
applying to Monte Carlo samples some scale factors derived 
with tag-and-probe techniques 
on $Z\to \ell^+\ell^-$ ($\ell=e,\mu$) data and simulated samples.
For each of these three sources of systematic uncertainty, 
the overall systematic uncertainty is obtained 
as the quadratic sum of the statistical
and systematic uncertainties on the corresponding scale factor.

In the electron channel, the systematic uncertainties corresponding to
electron reconstruction, identification and trigger, are 0.3\%, 1.1\% 
and 0.2\%, respectively.
In the muon channel, the systematic uncertainties corresponding to
muon reconstruction, identification and trigger, are 0.2\%, 1.1\% and 1.4\%, 
respectively.
A total uncertainty on the signal and background acceptances of 2\% is estimated.


\subsubsection{Lepton momentum scale and resolution}

To check the accuracy of the lepton momentum scale and resolution
simulated samples of $Z\to \ell^+\ell^-$ and $J/\psi \to \ell^+\ell^-$
are used to reconstruct the particles masses (for electrons also
 $W\to e\nu$ events are used from  $E/p$  studies). In the case of muons,
the small discrepancies observed between data and simulation are
corrected adjusting muon energy scale and resolution in the Monte Carlo
samples. In the case of electrons, energy resolution corrections
are applied only to Monte Carlo samples but energy scale corrections
are applied to data in all detector regions and to simulation only in 
the calorimeter transition region.
The systematic uncertainties on these scale factors are varied
separately and the result on the total yields are are at the 
sub-percent level and considered therefore negligible in the analyses.

\subsubsection{JVF efficiency}
\label{sec:syst_jvf}

Recalling the cut applied on the JVF variable (Section~\ref{sec:jets}) 
of $|{\rm JVF}|>0.5$, the per-jet efficiency of this requirement
is estimated in $Z(\to \ell^+\ell^-)$+1-jet events both in data and
Monte Carlo simulation. Events enriched in hard-scatter jets are selected
separately from events enriched in jets from pile-up interactions and
specific efficiency and inefficiency scale factors are measured.
Scale factors for pileup jets are estimated to be consistent with 1, while
efficiency scale factors for hard-scatter jets goes from $\sim$1.03 for jets with $\pt=25\gev$
down to $\sim$1.01 for jets with $\pt>150\gev$.
An overall event weight is obtained as the product of all per-jet scale factors
and is applied to the Monte Carlo samples.
The systematic uncertainty from the propagation of the per-jet scale
factor uncertainty gives an overall uncertainty on the signal
and background acceptance of $\sim$2.5\%.


\subsubsection{Jet energy scale}
\label{sec:syst_jes}

The systematic uncertainty on the Jet Energy Scale (JES) 
has been derived combining the information from both test-beam 
and collision data and Monte Carlo
simulation~\cite{jes, insitu5,insitu6}.  
Pile-up activity produces an additional source of systematic 
uncertainty that depends on the number of primary vertices
and on the average number of interactions per bunch crossing $<\mu>$. 
%The first estimate of the effect of pile-up on the jet energy scale uncertainty was developed in 2011~\cite{insitu6}, and subsequently validated with in situ momentum balance techniques in studies of the 2012 data.
Momentum balance techniques in $Z$+jets, $\gamma$+jets and 
multi-jet events are combined to derive a small residual correction
for jets in the transverse momentum range $20\gev<\pt\lesssim 1\tev$.

The overall variation due to JES systematic uncertainty 
evaluated in the central detector region 
is $\sim$4\% for jets with $\pt=25\gev$ and improves to $\sim$1\% for  
jets with $\pt=500\gev$~\cite{jesuncertainty}.
The effect of this systematic uncertainty is 
implemented in the analyses by varying in the Monte Carlo samples the 
transverse momentum of all the selected jets by $\pm$1 standard deviation.
In each event the missing transverse momentum \met\ is then corrected consistently to 
the varied $\pt$ of the jets and all the variables involving jets are also
recomputed.

As can be seen in Table~\ref{tab:SystSummary}, for the \htx\ analysis
the JES systematic uncertainty is split into eight
uncorrelated components, each with a different jet $\pt$ and $\eta$
dependence, which are treated independently.
The \wbx\ instead uses the total JES uncertainty as a single uncertainty
resulting from the sum in quadrature of all individual sources.
The reasons for this different treatment are explained in the
corresponding analysis chapters.


\subsubsection{Jet energy resolution}
\label{sec:syst_jer}

The Jet Energy Resolution (JER) was measured 
with two {\em in-situ} techniques~\cite{jes}
as a function of the jet transverse momentum and pseudo-rapidity.
It is consistent in data and Monte Carlo simulation and no corrections
are needed.
To account for the systematic uncertainty, the quadratic difference 
between the JER in data and in simulated samples is used
to smear the energy of jets in Monte Carlo simulation and a new
varied sample is obtained with a different normalisation and 
variable distributions shapes. The final result is then symmetrised
to obtain both positive and negative variations.
%In order to propagate the uncertainty in the $\pt$ resolution, for each jet in the simulation, a random number $r$ is drawn from a Gaussian distribution with mean 0 and sigma equal to the difference in quadrature between the fractional $\pt$ resolution with the tool and the nominal one.  The jet 4-momentum is then scaled by a factor $1+r$. This jet energy resolution uncertainty is assumed to be fully correlated point-by-point.
%The same rebinning algorithm as discussed in Section~\ref{sec:syst_jes} is applied.


\subsubsection{Heavy- and light-flavour tagging}
\label{sec:syst_btag}


The efficiencies in heavy flavour ($b$ and $c$) jets identification with
the $b$-tagging algorithm are measured in data and depend on the individual
jet flavour~\cite{btagging,ctagging,ltagging}.
These efficiencies are measured from data and depend on the jet flavour:
in Monte Carlo events $b$ ($c$) jet efficiencies are corrected with scale factors
of 0.9--1.0 (1.1--1.2) depending on $\pt$, light jet efficiencies are corrected 
with a  scale factor of $\sim$1.3.
Every jet in the Monte Carlo simulated events is corrected depending
on its flavour, $\pt$ and $\eta$.
The uncertainty on these scale factors is 
between 7\% and 13\% for $b$ jets, between 15\% and 39\% for $c$ jets,
and $\sim$25\% for light jets.

As was reported in Table~\ref{tab:SystSummary}, the systematic uncertainty
on \btag ging (\ctag ging) efficiency is divided into nine (five) independent 
components that correspond to an eigenvector from the diagonalization of the
matrix containing the information on the total uncertainty 
per $\pt$ bin and the bin-to-bin correlations (see Reference~\cite{VHbb} for
more details).
These individual sources of systematic uncertainties 
are taken as uncorrelated between $b$, $c$ jets, and
light flavour jets. In Monte Carlo simulated events 
a per-jet weighting procedure~\cite{IFAEBtagNote}
is applied in order to propagate the \btag ging calibration
and related uncertainties.

\subsection{Theoretical cross-sections}
\label{sec:syst_bkgxsect}

Normalization-only systematic uncertainties on the theoretical
cross-sections are considered as follows:
+10\%/-11\% for the inclusive $t\bar{t}$
production cross section evaluated at approximate NNLO using 
\texttt{HATHOR}~\cite{ttbarxs}; +5\%/-4\% and $\pm 5\%$ 
for the theoretical cross sections of the single
top~\cite{stopxs,stopxs_2} and diboson~\cite{dibosonxs} backgrounds
respectively; +12\%/-17\% and  $\pm 30\%$ 
for the theoretical cross sections of the $t\bar{t}H$~\cite{lhcxs} and 
$t\bar{t}V$~\cite{ttbarVxs1,ttbarVxs2} backgrounds respectively.


\subsection{Normalizations of data-driven backgrounds and background modeling}
\label{sec:syst_norm}

Because of the differences between the effects of these systematic uncertainties
in the \wbx\ and \htx\ analyses, the reader is referred to the specific 
Sections~\ref{sec:wbxSYS} and~\ref{sec:htxSYS}


%\input{vlq_analysis/wbx}
%\input{vlq_analysis/htx}


\section{Statistical analysis}\label{sec:cls}

To test the presence or absence of signals from new physics we use the
\cls{s}\ method~\cite{cls,cls_2} originally developed in the context of Higgs
searches at the LEP collider~\cite{Read:451614}.
The fundamental principle of this technique is that, in order to 
exclude or verify a theory predicting some kind of signal over some other kind
of background, both ``background only'' and ``signal plus background''
hypotheses, denoted by $\mathcal{H}_{b}$ and $\mathcal{H}_{s+b}$ 
respectively, have to be tested. Once a test statistic $Q$ has been chosen
for the analysis, two confidence levels (CL) are defined for the two hypotheses:
\begin{eqnarray}\label{eq:clbclsb}
\cls{b} &=& P(Q \leq Q_{\rm obs}|\mathcal{H}_{b}),\\
\cls{s+b} &=& P(Q \leq Q_{\rm obs}|\mathcal{H}_{s+b}),
\end{eqnarray}
with $Q_{\rm obs}$ being the observed value in data of the 
test statistic. %It is good to identify well separated test statistic distributions in order to be able to clearly distinguish between  events consistent with the backgroud only prediction and events showing deviations consistent with the signal+background hypothesis.


The \cls{s}\ is then defined as the ratio of the two above confidence levels:
\begin{equation}\label{eq:cls}
\cls{s} = \cls{s+b}/\cls{b}
\end{equation}
and its meaning is given by its relation with the confidence 
level $CL$ for the exclusion of the signal hypothesis 
$\cls{s} \leq 1 - CL$. 
%i.e. values of $\cls{s}<0.05$ 
For instance, signal hypotheses for which
$\cls{s} < 0.05$ are deemed to be excluded at the 95\% CL.
Using $\cls{s}$ instead of $\cls{s+b}$ minimizes the possibility 
of mistakenly excluding a small signal due 
to a downward background fluctuation, 
as this would lead to small values of both $\cls{s+b}$ and $\cls{b}$.

Similarly, the confidence level for excluding the background only
hypothesis is the $p$-value $1 - \cls{b}$ and the value required to claim 
discovery is of $\sim 10^{-7}$. The motivation that led to
the definition of \cls{s} instead of using \cls{s+b}, which also
corresponds to the confidence level in excluding the signal+background
hypothesis, is that the latter might wrongly exclude scenarios to
which the analysis is simply non-sensitive to like is the general case
for searches for rare events.

\begin{figure}[htb]\begin{center}
        \psfrag{AAA}{\tiny $P(Q|\mathcal{H}_{b})$}
        \psfrag{BBB}{\tiny $P(Q|\mathcal{H}_{s+b})$}
        \psfrag{CCC}{\tiny $1 - CL_{b}$}
        \psfrag{DDD}{\tiny $CL_{s+b}$}
        \psfrag{XXX}{$Q$}
        \psfrag{YYY}{\hskip-10ex Probability density}
	\subfigure[]{\label{fig:sepLLR}
  	\includegraphics[width=0.45\textwidth]{vlq_analysis/figures/separatedLLR}}
	\subfigure[]{\label{fig:ovrLLR}
  	\includegraphics[width=0.45\textwidth]{vlq_analysis/figures/overlappedLLR}}
	\caption{Example of (a) sensitive and (b) not sensitive analyses using the test 
        statistics $Q$, often chosen as a log-likelihood.\label{fig:LLR}} %$-2\log Q$.\label{fig:LLR}}
\end{center}\end{figure}


For the two analyses presented in this dissertation, the test statistic
is defined as a log-likelihood ratio 
\begin{equation}\label{eq:llr}
Q \equiv -2\log \dfrac{\mathcal{L}({\rm data}|\mathcal{H}_{s+b})}{\mathcal{L}({\rm data}|\mathcal{H}_{b})},
\end{equation}
where the likelihoods $\mathcal{L}({\rm data}|\mathcal{H}_{s+b})$
 and $\mathcal{L}({\rm data}|\mathcal{H}_{b})$ for 
signal+background and background only hypothesis
are built from the chosen discriminant variable distribution 
as the bin by bin product of Poisson probabilities to observe the
data under one or the other hypothesis.
An example of the probability density distribution
for the test statistics is given in Figure~\ref{fig:LLR},
where the two distributions in the signal+background and background only hypothesis
have a different overlay: while in Figure~\ref{fig:sepLLR} the experiment
result (the vertical line) will fall explicitly either
in one or the other distributions, in Figure~\ref{fig:ovrLLR} the
outcome is not clear. These are examples of sensitive and not sensitive searches,
respectively.

Eventually more than one channel can be combined (like will be
the case in the \htx\ analysis) and the complete formula for
the likelihood computation reads:
\begin{align}
-2 \log {\cal L}({\rm data}|\mathcal{H}_x) 
   & =  
   %=
   -2 \log {\cal L}(\vec{n}|R,\vec{s},\vec{b},\vec{\theta}) \notag\\
   %=
   & =  
  -2 \sum_{i=1}^{N_{chan}}\sum_{j=1}^{N_{bins}} (n_{ij}\log \mu_{ij}-\mu_{ij})+\sum_{k=1}^{N_{par}} \theta_k^2.
\end{align}
Here the first sum is over the number of channels
combined in the analysis $N_{chan}\geq 1$ and the 
second sum is over the number of the discriminant variable
histogram bins $N_{bins}$. $n_{ij}$ ($\mu_{ij}$) is the 
number of events in data (expected number of events) 
for channel $i$ and histogram bin $j$. $\mu_{ij}$ is given by
$\mu_{ij} = R s_{ij}(\vec{\theta})+ b_{ij}(\vec{\theta})$, 
where $s_{ij}$  and $b_{ij}$ represent the
expected signal and background yields, 
the first being equal to zero in the background only hypothesis.
$R$ is a scaling 
parameter applied to the signal
to test the sensitivity of the search and $\vec{\theta}$
are the nuisance parameters parametrizing the effect of
systematic uncertainties. 
The statistical uncertainty of the Monte Carlo samples is 
also taken into account when computing our likelihoods as
an uncertainty of the templates, which in the case of
the non-$t\bar{t}$ background are merged into a single
template  with different weights.
%The {\sc mclimit} flag used is 2, meaning we consider the uncertainties as given by the uncertainties of the templates. 
%This allows us to correctly estimate the statistical uncertainty when many templates are merged together with different weights, such as in the case of the merged non-$t\bar{t}$ background.
%These parameters are assigned Gaussian penalty terms in the likelihood corresponding
%to their nominal uncertainties.
For both hypotheses pseudoexperiments are generated to
account in each bin for statistical fluctuations (Poisson-distributed)
and systematic variations (Gaussian-distributed). The effect of
systematic uncertainties are described by nuisance parameters taken
at their nominal values and no parameter fitting is performed.
In the case of the \htx\ analysis it will be explained that
two additional nuisance parameters are introduced and fitted
to help reduce the degradation in sensitivity degradation due to a poor
Monte Carlo modeling of the \ttbar+HF component.


The systematic uncertainties are treated inside the \mclimit\ 
package~\cite{mclimitATLAS} developed for ATLAS heavy quark
searches based on the original \mclimit\ code~\cite{Heinrich:7587,Junk:8128,Junk:7904}.
Here the  histograms are interpolated between the nominal and the 
systematically shifted templates bin-by-bin, with a shift of $+0.5\sigma$
corresponding to half way between the nominal and the $+1\sigma$ shifted template.
This interpolation method is called vertical morphing and uses a linear 
bin-by-bin interpolation, but for variations below $1\sigma$ we use
quadratic interpolation to ensure a continuous derivative at zero shift.
Pseudoexperiments are generated using these interpolated numbers for
all systematic uncertainties.


%In absence of data excess over background prediction, signal cross section values yielding $\cls{s}\leq 0.05$ are considered to be exclude at $\geq$95\% CL.

